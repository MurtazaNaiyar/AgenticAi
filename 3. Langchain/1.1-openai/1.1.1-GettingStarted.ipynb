{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001C8A45018E0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001C8A46AADE0> root_client=<openai.OpenAI object at 0x000001C8A41B1970> root_async_client=<openai.AsyncOpenAI object at 0x000001C8A41F5640> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative AI refers to a subset of artificial intelligence techniques that focus on creating new content, such as text, images, music, and other media, by learning patterns from existing data. At its core, generative AI models aim to understand the distribution and structure of the training data to generate similar but novel outputs.\\n\\nSome key technologies and models associated with generative AI include:\\n\\n1. **Generative Adversarial Networks (GANs):** These consist of two neural networks—a generator and a discriminator—that are trained together. The generator creates new data instances, while the discriminator evaluates their authenticity, distinguishing between real and generated data. This process helps the generator improve over time.\\n\\n2. **Variational Autoencoders (VAEs):** VAEs are another type of generative model that encode input data into a latent space and then decode it back to reconstruct the original data. They are particularly good at learning the underlying structure of data.\\n\\n3. **Transformers:** Originally designed for sequence-to-sequence tasks in natural language processing, transformers have been adapted for generative tasks. Models like GPT (Generative Pre-trained Transformer) use this architecture to generate human-like text based on a given prompt.\\n\\n4. **Diffusion Models:** These are a class of generative models that learn to generate data by iteratively refining noisy examples into coherent outputs, often used in image generation.\\n\\nGenerative AI has numerous applications across different industries, including:\\n\\n- **Content Creation:** Automatically generating written content, images, or music.\\n  \\n- **Design and Art:** Assisting artists and designers by providing tools that generate new visuals or designs.\\n\\n- **Healthcare:** Creating synthetic medical data for research and training purposes.\\n\\n- **Gaming and Entertainment:** Developing characters, levels, or scenarios in video games.\\n\\n- **Data Augmentation:** Enhancing and expanding training datasets for other machine learning models.\\n\\nGenerative AI presents both exciting opportunities and challenges, particularly concerning ethical considerations like deepfakes, copyright issues, and the potential for misuse. As the technology evolves, it's important for developers and policymakers to address these concerns proactively.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 419, 'prompt_tokens': 13, 'total_tokens': 432, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BeRl7xttiFvhAJMxKTLiE57CS7oZr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0626bb63-c244-4ff3-bf07-36661a06dd32-0' usage_metadata={'input_tokens': 13, 'output_tokens': 419, 'total_tokens': 432, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert AI Engineer. Provide me answers based on the questions')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Certainly! Langsmith is a tool designed to enhance the development and debugging of applications that utilize large language models (LLMs) and AI chains. It offers several key features to streamline the development process:\\n\\n1. **Logging and Tracing**: Langsmith provides robust logging and tracing capabilities, allowing developers to gain insights into the behavior of their LLM applications. This helps in understanding how data flows through the system and identifying any issues or bottlenecks.\\n\\n2. **Testing**: It offers tools to test the performance and reliability of LLMs and chains. This includes the ability to run various test scenarios and assess how the models perform under different conditions.\\n\\n3. **Monitoring**: Langsmith includes monitoring features to keep track of the application's performance in real-time. This is crucial for maintaining the reliability and efficiency of LLM applications in production environments.\\n\\n4. **Integrations**: The tool can be integrated with various platforms and services, making it a versatile addition to the developer's toolkit. This includes support for different LLM providers and development frameworks.\\n\\n5. **User Interface**: Langsmith often comes with a user-friendly interface that simplifies the process of managing and debugging LLM applications. This can be particularly beneficial for developers who are not experts in AI but need to work with LLMs.\\n\\nOverall, Langsmith acts as a comprehensive solution for developers looking to build, test, and maintain applications that leverage large language models and AI chains effectively.\" response_metadata={'token_usage': {'completion_tokens': 292, 'prompt_tokens': 33, 'total_tokens': 325}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_f4e629d0a5', 'finish_reason': 'stop', 'logprobs': None} id='run-cfbfb9b0-0c8f-4712-997a-a36c0319d1e0-0' usage_metadata={'input_tokens': 33, 'output_tokens': 292, 'total_tokens': 325}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! As of my knowledge cutoff date in October 2023, \"Langsmith\" does not directly correlate to any widely recognized technology, framework, or concept in the field of AI or software engineering. It's possible that it might be a term used within a specific company, a proprietary tool, or a recent development that hasn't gained widespread recognition.\n",
      "\n",
      "However, if \"Langsmith\" is a term related to a specific context or a new product or service that emerged after my last update, I wouldn't have information on it. If you could provide more context or details, I'd be happy to help you understand the concept or direct you to the right resources!\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
